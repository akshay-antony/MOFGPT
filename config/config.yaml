random_seed: 42
project_name: "llama2_mof_gpt"
data:
  csv_folder_paths: 
    - '../benchmark_datasets/QMOF/mofid/'
    # - '../benchmark_datasets/hMOF/mofid/'
    # - '../benchmark_datasets/Boyd&Woo/mofid/'
  vocab_path: "../tokenizer/vocab_with_eos.txt"
  train_test_ratio: 0.8
  batch_size: 32 #4
  num_workers: 8 #1
  ignore_index: -100
  use_multiprocessing: False
  tokenizer:
    max_seq_len: 512
    pad_token: '[PAD]'
    mask_token: '[MASK]'
    bos_token: '[BOS]'
    eos_token: '[EOS]'
    unk_token: '[UNK]'
    add_special_tokens: True
    truncation: True

model:
  model_name: "llama2"
  pretrained_model_name: "llama2"
  hidden_size: 1024
  intermediate_size: 2048
  num_hidden_layers: 8
  num_attention_heads: 8
  num_key_value_heads: 8 # for grouped query attention
  use_cache: False
  rope_theta: 10000.0 # default value
  hidden_act: "silu" # default value
  # return vals
  return_dict: True
  output_attentions: False
  output_hidden_states: False

training:
  device: "cuda"
  epochs: 10
  fp16: True
  top_ks: [1, 3, 5]
  batch_size: 32 #4
  optimizer:
    lr: 0.0001
    type: "AdamW"
    weight_decay: 0.001
    gradient_accumulation_steps: 1
  scheduler:
    type: "cosine"
    warmup_ratio: 0.03
  logging_ratio: 0.05
  save_ratio: 0.5
  save_dir: "../saved_models/"

reinforcement_learning:
  training:
    saved_state_dict_filename: "../saved_models/llama2_best.pt"
    saved_state_dict_url: "https://drive.google.com/file/d/1uuOjMQJc5seEPkak7gRse4N5HSssmhVp/view?usp=sharing"
    num_samples_per_epoch: 2048
    epochs: 10
    fp16: True
    optimizer:
      lr: 0.0001
      type: "AdamW"
      weight_decay: 0.001
    logging_ratio: 0.05
    save_ratio: 0.5
    save_dir: "../saved_models/"
    model_name: "llama2_rl"
    batch_size: 128
  eval:
    num_samples: 8
    eval_interval: 1
    print_smiles: True
  model:
    model_name: "llama2_rl"
    use_cache: False
    return_dict: True
    output_attentions: False
    output_hidden_states: False
  sampling:
    max_seq_len: 512
    batch_size: 128
    do_sample: True
    top_k: 1000
    top_p: 0.95
    temperature: 0.7
    constant_temperature: False
    temperature_std: 1.0
    early_stopping: False
    num_beams: 1
    num_beam_groups: 1
  reward:
    sep_token: "&&"
    name: "basic_rules"
    discount_factor: 0.99
    basic_rules:
      single_sep_reward: 1.5
      multiple_sep_reward: -1
      no_sep_reward: -1.5
      eos_reward: 0.5
      no_eos_reward: -0.5


  
