random_seed: 42
project_name: "llama2_mof_gpt"
data:
  csv_folder_paths: 
    - './benchmark_datasets/QMOF/mofid/'
    # - './benchmark_datasets/hMOF/mofid/'
    # - './benchmark_datasets/Boyd&Woo/mofid/'
  vocab_path: "./tokenizer/vocab_with_eos.txt"
  train_test_ratio: 0.8
  batch_size: 4
  num_workers: 4
  ignore_index: -100
  tokenizer:
    max_seq_len: 512
    pad_token: '[PAD]'
    mask_token: '[MASK]'
    bos_token: '[BOS]'
    eos_token: '[EOS]'
    unk_token: '[UNK]'
    add_special_tokens: True
    truncation: True

model:
  model_name: "llama2"
  pretrained_model_name: "llama2"
  hidden_size: 1024
  intermediate_size: 2048
  num_hidden_layers: 8
  num_attention_heads: 8
  num_key_value_heads: 8 # for grouped query attention
  use_cache: False
  rope_theta: 10000.0 # default value
  hidden_act: "silu" # default value
  # return vals
  return_dict: True
  output_attentions: False
  output_hidden_states: False

training:
  device: "cuda"
  epochs: 10
  fp16: True
  top_ks: [1, 3, 5]
  optimizer:
    lr: 0.0001
    type: "AdamW"
    weight_decay: 0.001
    gradient_accumulation_steps: 1
  scheduler:
    type: "cosine"
    warmup_ratio: 0.03
  logging_ratio: 0.05
  save_ratio: 0.5
  save_dir: "./saved_models/"



  
